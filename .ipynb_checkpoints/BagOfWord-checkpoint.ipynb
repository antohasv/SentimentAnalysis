{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 8836: expected 4 fields, saw 5\n",
      "\n",
      "Skipping line 535882: expected 4 fields, saw 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_post = pd.read_csv(\"Sentiment_Analysis_Dataset.csv\", header=0, delimiter=\",\", error_bad_lines=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_post = all_post.drop(\"SentimentSource\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_post = all_post.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#slice posts in two part: train and test. Get 10% for test.\n",
    "test_len = int(round(all_post.shape[0] * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157861"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = all_post[0: 20000]#[test_len:]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = all_post[20000: 40000]#[:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = test_df.SentimentText[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1</td>\n",
       "      <td>- Fwd: Good Morn,happy birthday !  regardless ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>0</td>\n",
       "      <td>gah. so much less ok than i was trying to tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0</td>\n",
       "      <td>GCSE's clearly suck.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0</td>\n",
       "      <td>- Geez Chelsea already scored against Everton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0</td>\n",
       "      <td>gerald lost a friend............idk what to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0</td>\n",
       "      <td>getting &amp;quot;goodbye&amp;quot; e-mails from #Ira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1</td>\n",
       "      <td>-- getting a Mani + Pedi with the husband!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1</td>\n",
       "      <td>getting a webcam today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0</td>\n",
       "      <td>- Getting ready to leave for Spring City, TN -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1</td>\n",
       "      <td>- getting ready to watch the Sprint Cup boys t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0</td>\n",
       "      <td>girls girls. don't fight! you both can have m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0</td>\n",
       "      <td>gnight my phone sucks Bleh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1</td>\n",
       "      <td>-- g'night tweeties! &amp;lt;3 talk to you all in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0</td>\n",
       "      <td>go away migraine!!! you are not welcome!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0</td>\n",
       "      <td>go away rain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1</td>\n",
       "      <td>go follow my personal twitter! @nikkidallasen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1</td>\n",
       "      <td>Go Wings! FTW!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1</td>\n",
       "      <td>- God i'm up early. Hayley still asleep but to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1</td>\n",
       "      <td>going home to see my baby   [dont be ashamed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1</td>\n",
       "      <td>going out for dinner with the family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0</td>\n",
       "      <td>going to bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1</td>\n",
       "      <td>going to bed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1</td>\n",
       "      <td>- going to bed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>0</td>\n",
       "      <td>going to get piï¿½a colada mix to wallow in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0</td>\n",
       "      <td>going to have such a bad day, I love my x3 I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0</td>\n",
       "      <td>going to lay down. Fuck tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1</td>\n",
       "      <td>Going to London Expo this weekend  Can't wait !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0</td>\n",
       "      <td>going to master classes without my car... I m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1</td>\n",
       "      <td>- goingg to sleep  school , then out for pizzaa!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0</td>\n",
       "      <td>gonna miss jesse mccartney's live stream from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0</td>\n",
       "      <td>i... have to start... work... tomorrow..........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>1</td>\n",
       "      <td>icant stop smiling! Especially thinking of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0</td>\n",
       "      <td>idk why i 'm so sad  i need to somone right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0</td>\n",
       "      <td>- Ii cant beleivee the ppl that imm losinng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0</td>\n",
       "      <td>i'll be leaving home tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>0</td>\n",
       "      <td>ill deff be going up early on Tuesday will yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0</td>\n",
       "      <td>I'll get home like 5pm today it will be a lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1</td>\n",
       "      <td>I'll get my son Tommi to visit at me tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm a sad panda today. I want a slurpee damnit!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm about to turn down a great job offer in L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1</td>\n",
       "      <td>im almost done with this portrait. woo. hmm.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm back online!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1</td>\n",
       "      <td>im back, but now going offline seeya guys  ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0</td>\n",
       "      <td>im being attacked by a moth!!  its scary stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm being confused..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>0</td>\n",
       "      <td>i'm board :b and happy for tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm Bored... Bored....Bored Again And Develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm boreddddd...rain rain go away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>0</td>\n",
       "      <td>im buzzed and followed friends to punk ass ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0</td>\n",
       "      <td>Im crushed, How could i have been so stupid?....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm depressed about the cavs loss now I'm che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0</td>\n",
       "      <td>Im dying of thirst and I wish I was in a larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm exactly where I want to be. ))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm exhausted. Having pasta soon, thank god.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1</td>\n",
       "      <td>im followin demi she rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm freaking out    ~ I got something to admit ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>im fukin bored im goin on gs 4 a bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1</td>\n",
       "      <td>finally awake!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1</td>\n",
       "      <td>- finally catching the new episode of college ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "      <td>finally finished. now i can sleep peacefully!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                      SentimentText\n",
       "1000          1  - Fwd: Good Morn,happy birthday !  regardless ...\n",
       "1001          0   gah. so much less ok than i was trying to tel...\n",
       "1002          0                               GCSE's clearly suck.\n",
       "1003          0  - Geez Chelsea already scored against Everton ...\n",
       "1004          0   gerald lost a friend............idk what to s...\n",
       "1005          0   getting &quot;goodbye&quot; e-mails from #Ira...\n",
       "1006          1        -- getting a Mani + Pedi with the husband! \n",
       "1007          1                            getting a webcam today.\n",
       "1008          0  - Getting ready to leave for Spring City, TN -...\n",
       "1009          1  - getting ready to watch the Sprint Cup boys t...\n",
       "1010          0   girls girls. don't fight! you both can have m...\n",
       "1011          0                         gnight my phone sucks Bleh\n",
       "1012          1  -- g'night tweeties! &lt;3 talk to you all in ...\n",
       "1013          0         go away migraine!!! you are not welcome!!!\n",
       "1014          0                                      go away rain.\n",
       "1015          1      go follow my personal twitter! @nikkidallasen\n",
       "1016          1                                     Go Wings! FTW!\n",
       "1017          1  - God i'm up early. Hayley still asleep but to...\n",
       "1018          1   going home to see my baby   [dont be ashamed ...\n",
       "1019          1              going out for dinner with the family \n",
       "1020          0                                       going to bed\n",
       "1021          1                                      going to bed.\n",
       "1022          1                                   - going to bed. \n",
       "1023          0   going to get piï¿½a colada mix to wallow in m...\n",
       "1024          0   going to have such a bad day, I love my x3 I ...\n",
       "1025          0                    going to lay down. Fuck tonight\n",
       "1026          1    Going to London Expo this weekend  Can't wait !\n",
       "1027          0   going to master classes without my car... I m...\n",
       "1028          1   - goingg to sleep  school , then out for pizzaa!\n",
       "1029          0   gonna miss jesse mccartney's live stream from...\n",
       "...         ...                                                ...\n",
       "1970          0   i... have to start... work... tomorrow..........\n",
       "1971          1   icant stop smiling! Especially thinking of th...\n",
       "1972          0   idk why i 'm so sad  i need to somone right now \n",
       "1973          0       - Ii cant beleivee the ppl that imm losinng \n",
       "1974          0                     i'll be leaving home tomorrow.\n",
       "1975          0   ill deff be going up early on Tuesday will yo...\n",
       "1976          0   I'll get home like 5pm today it will be a lon...\n",
       "1977          1   I'll get my son Tommi to visit at me tomorrow...\n",
       "1978          0    I'm a sad panda today. I want a slurpee damnit!\n",
       "1979          0   I'm about to turn down a great job offer in L...\n",
       "1980          1   im almost done with this portrait. woo. hmm.....\n",
       "1981          1                                   I'm back online!\n",
       "1982          1   im back, but now going offline seeya guys  ha...\n",
       "1983          0   im being attacked by a moth!!  its scary stuf...\n",
       "1984          0                               I'm being confused..\n",
       "1985          0                i'm board :b and happy for tomorrow\n",
       "1986          0   I'm Bored... Bored....Bored Again And Develop...\n",
       "1987          0                 I'm boreddddd...rain rain go away.\n",
       "1988          0   im buzzed and followed friends to punk ass ba...\n",
       "1989          0   Im crushed, How could i have been so stupid?....\n",
       "1990          0   I'm depressed about the cavs loss now I'm che...\n",
       "1991          0   Im dying of thirst and I wish I was in a larg...\n",
       "1992          1                 I'm exactly where I want to be. ))\n",
       "1993          0       I'm exhausted. Having pasta soon, thank god.\n",
       "1994          1                         im followin demi she rocks\n",
       "1995          0   I'm freaking out    ~ I got something to admit ~\n",
       "1996          0               im fukin bored im goin on gs 4 a bit\n",
       "1997          1                                  finally awake!!!!\n",
       "1998          1  - finally catching the new episode of college ...\n",
       "1999          1      finally finished. now i can sleep peacefully!\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",  # :D in lower case\n",
    "    \":dd\": \" good \",  # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "\n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "}\n",
    "\n",
    "emo_repl_order = [s for (lenk, s) in reversed(sorted([(len(k), k)for k in emo_repl.keys()]))]\n",
    "\n",
    "def clean_post(raw_post):\n",
    "    import HTMLParser \n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    #Download package stopwords and stemmer use:\n",
    "    #import nltk\n",
    "    #nltk.download()\n",
    "    \n",
    "    #url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    #raw_post = re.sub(url_pattern, '', raw_post, flags=re.MULTILINE)\n",
    "    for k in xrange(0, len(emo_repl)):\n",
    "        raw_post = raw_post.replace(k, emo_repl[k])\n",
    "    \n",
    "    raw_post = HTMLParser.HTMLParser().unescape(raw_post.decode('utf-8')) #&quot - to symbol representation\"\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_post)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = lower_case.split()\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':&gt;',\n",
       " '&lt;3',\n",
       " ':dd',\n",
       " ':-S',\n",
       " ':-)',\n",
       " ':-(',\n",
       " \":')\",\n",
       " '(-:',\n",
       " ';)',\n",
       " ':d',\n",
       " ':S',\n",
       " ':/',\n",
       " ':)',\n",
       " ':(',\n",
       " '8)',\n",
       " '(:']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_posts(raw_posts):\n",
    "    clean_posts = []\n",
    "    for i in xrange(0, raw_posts.SentimentText.size):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\"{0} completed\".format(i + 1))\n",
    "        clean_posts.append(clean_post(raw_posts.SentimentText[i]))\n",
    "    return clean_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bag_of_words(clean_posts):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "    data_features = vectorizer.fit_transform(clean_posts)\n",
    "    #vocab = vectorizer.get_feature_names()\n",
    "    return data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected a character buffer object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-0dfaf2f11ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_train_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_posts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_train_posts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c9872acace9f>\u001b[0m in \u001b[0;36mclean_posts\u001b[0;34m(raw_posts)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} completed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mclean_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentimentText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-cfed150680bc>\u001b[0m in \u001b[0;36mclean_post\u001b[0;34m(raw_post)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#raw_post = re.sub(url_pattern, '', raw_post, flags=re.MULTILINE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memo_repl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mraw_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_repl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mraw_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#&quot - to symbol representation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected a character buffer object"
     ]
    }
   ],
   "source": [
    "clean_train_posts = clean_posts(train_df)\n",
    "data_train_features = create_bag_of_words(clean_train_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':&gt;',\n",
       " '&lt;3',\n",
       " ':dd',\n",
       " ':-S',\n",
       " ':-)',\n",
       " ':-(',\n",
       " \":')\",\n",
       " '(-:',\n",
       " ';)',\n",
       " ':d',\n",
       " ':S',\n",
       " ':/',\n",
       " ':)',\n",
       " ':(',\n",
       " '8)',\n",
       " '(:']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit(data_train_features, train_df.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 completed\n",
      "20000 completed\n"
     ]
    }
   ],
   "source": [
    "clean_test_posts = clean_posts(test_df)\n",
    "data_test_features = create_bag_of_words(clean_test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = forest.predict(data_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['Sentiment_Result'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['Sum_Result'] = (test_df.Sentiment + test_df.Sentiment_Result) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4977"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(test_df.Sum_Result) / (np.size(test_df.Sum_Result) * 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "#dist = np.sum(train_data_features, axis=0)\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2= model.fit(data_train_features, train_df.Sentiment).predict(data_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeline = pd.read_csv(\"timeline.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>672190736373325824</td>\n",
       "      <td>RT @strategyzer: Train 10+ ppl from your team ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>672185139992989696</td>\n",
       "      <td>Make sure to check the Proposed Schedule Chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>672184920660426752</td>\n",
       "      <td>Coffee &amp;amp; Ice Cream are now being served in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>672180762163019776</td>\n",
       "      <td>RT @waynebeaton: Visualizing Java 9 Module Rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>672177601566347264</td>\n",
       "      <td>Afternoon Sponsored Sessions are now underway!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>672166930267152385</td>\n",
       "      <td>List map by @Flat_Studio on @dribbble https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>672164982163816448</td>\n",
       "      <td>Track every mile &amp;amp; moment to breaking thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>672164596153606144</td>\n",
       "      <td>Come join #google at the 10th #andevcon Women ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>672163381160226816</td>\n",
       "      <td>RT @brunoborges: To celebrate 10,000+ follower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>672159509159256064</td>\n",
       "      <td>Lunch is served in Grand Ballroom B and the Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>672158523279605760</td>\n",
       "      <td>If you're wondering what the stream was all ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>672158219230306304</td>\n",
       "      <td>A9: Once you get \"Hello World\" on your phone t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>672158051806220288</td>\n",
       "      <td>A9: Great question! Don't be intimidated: Get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>672157722700218368</td>\n",
       "      <td>Circuits &amp;amp; Electronics with @MIT Professor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>672156732185939969</td>\n",
       "      <td>A8: ...supporting such a large, diverse, and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>672156252265308161</td>\n",
       "      <td>A8: At Google we support them through docs, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>672155864464101377</td>\n",
       "      <td>A8: We love the Android dev community! New dev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetId                                          TweetText\n",
       "0   672190736373325824  RT @strategyzer: Train 10+ ppl from your team ...\n",
       "1   672185139992989696  Make sure to check the Proposed Schedule Chang...\n",
       "2   672184920660426752  Coffee &amp; Ice Cream are now being served in...\n",
       "3   672180762163019776  RT @waynebeaton: Visualizing Java 9 Module Rel...\n",
       "4   672177601566347264  Afternoon Sponsored Sessions are now underway!...\n",
       "5   672166930267152385  List map by @Flat_Studio on @dribbble https://...\n",
       "6   672164982163816448  Track every mile &amp; moment to breaking thro...\n",
       "7   672164596153606144  Come join #google at the 10th #andevcon Women ...\n",
       "8   672163381160226816  RT @brunoborges: To celebrate 10,000+ follower...\n",
       "9   672159509159256064  Lunch is served in Grand Ballroom B and the Wo...\n",
       "10  672158523279605760  If you're wondering what the stream was all ab...\n",
       "11  672158219230306304  A9: Once you get \"Hello World\" on your phone t...\n",
       "12  672158051806220288  A9: Great question! Don't be intimidated: Get ...\n",
       "13  672157722700218368  Circuits &amp; Electronics with @MIT Professor...\n",
       "14  672156732185939969  A8: ...supporting such a large, diverse, and g...\n",
       "15  672156252265308161  A8: At Google we support them through docs, on...\n",
       "16  672155864464101377  A8: We love the Android dev community! New dev..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
